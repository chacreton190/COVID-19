{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converts the JHU Cumulative Case Count Data to Daily Case Count Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "- [X] Figure out how to fix the control measure obs that are recorded as '.'\n",
    "- [X] Need to explore the missingness of the Oxford data. Sort the countries by GDP and examine what the missingness matrix looks like. **If you could run imputation on this data then you would have a major leg up on the other paper working on the similar topic. (on to of the other benefits to your paper)**\n",
    "- [X] Write the code that merges in the time series data for the diffent control measures\n",
    "- [X] Write the code merges in the Country Safety Index data\n",
    "- [X] Continue to update this **[file](https://1drv.ms/x/s!AjWX5HOdYY23kf9x5S7g8LKLGlseVg?e=992nsi)** of data source locations \n",
    "- [X] Write the code that lets you convert the US data to long\n",
    "- [X] Write the code that converts the column names in the Oxford data set to match the column names in the JHU dat\n",
    "- [X] Write the code that merges the countries to their offical alpha 3 code in the JHU: **[Link to Codes](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes)**\n",
    "- [X] **ALTERNATIVE TO ABOVE** use python-Levenshtein [Docs](https://rawgit.com/ztane/python-Levenshtein/master/docs/Levenshtein.html) distance to match similar country names \n",
    "> would still need to pair the high probability matches \n",
    "- [X] Write the code transposes the combined data with the control measures included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "import re"
=======
    "import re\n",
    "import numpy as np\n",
    "from functools import partial, reduce"
>>>>>>> Stashed changes
=======
    "import re\n",
    "import numpy as np\n",
    "from functools import partial, reduce"
>>>>>>> Stashed changes
=======
    "import re\n",
    "import numpy as np\n",
    "from functools import partial, reduce"
>>>>>>> Stashed changes
=======
    "import re\n",
    "import numpy as np\n",
    "from functools import partial, reduce"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "def long_maker(dset, var):\n",
    "    name_list = dset.columns.to_list()[1:]\n",
    "    long_df = pd.DataFrame(columns = {'date', var, 'country'})\n",
    "    for name in name_list:\n",
    "        df = dset.filter(items={name, 'date'})\n",
    "        df['country'] = name\n",
    "        df.rename(columns={name:var},inplace = True)\n",
    "        long_df = pd.concat([long_df, df],axis=0)\n",
    "    long_df.date = pd.to_datetime(long_df.date)\n",
    "    return long_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correcting_Col_Names(infile_path, dset):\n",
    "    infile = pd.read_csv(infile_path)\n",
    "    # og_col_list = infile.columns.to_list()\n",
    "    df = pd.DataFrame(infile.columns.to_list())\n",
    "    df['col'] = df[2:].apply(lambda x: pd.to_datetime(x).dt.strftime('X%m/X%d/%Y').str.replace('X0','').str.replace('X',''))\n",
    "    df['col'][0] = 'country'\n",
    "    df['col'][1] = 'country_code'\n",
    "    cols = df.col.to_list()\n",
    "    control = pd.read_csv(infile_path, names = cols, skiprows={0:1})\n",
    "    indexNames = control[ control['country_code'].isna()].index\n",
    "    control.drop(indexNames, inplace=True)\n",
    "    df = controls_transpose(control, dset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controls_transpose(dset, var):\n",
    "    import re\n",
    "    df = dset.T\n",
    "    df.columns = df.iloc[0]\n",
    "    df.columns =df.columns.str.lower()\n",
    "    df.drop(axis=0, index = {'country', 'country_code'},inplace = True)\n",
    "    df = df.reset_index().rename(columns={'index':'date'})\n",
    "    df = long_maker(df, var)\n",
    "    df.loc[df['country'].str.contains('cape verde',flags= re.IGNORECASE), 'country']= 'cabo verde'\n",
    "    df.loc[df['country'].str.contains('taiwan',flags= re.IGNORECASE), 'country']= 'taiwan'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_merger(df):\n",
    "    global long_df\n",
    "    long_df = long_df.merge(df, on=['date', 'country'],how='outer')\n",
    "    return long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "def stats_col_renamer(dframe):\n",
    "    \"\"\"\n",
    "    Tips: This fuction will remove special characters from column headers, replace spaces with columns, \n",
    "    and make all heading lower case\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dframe : TYPE Pandas dataframe\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dframe.columns = dframe.columns.str.lower()\n",
    "    dframe.columns = dframe.columns.str.replace('\\s{2,}',' ',regex=True).str.replace('-',' ').str.replace(' ','_').str.replace('[^A-Za-z0-9_]+','',regex=True)\n",
    "    return dframe\n",
    "data_path = r'..\\csse_covid_19_data\\csse_covid_19_time_series'\n",
    "out_data_path = r'..\\Modified Data Sets'\n",
    "control_data_path = '..\\Control Data'\n",
    "case_pre = pd.read_csv(f'{data_path}/time_series_covid19_confirmed_global.csv')\n",
    "death_pre = pd.read_csv(f'{data_path}/time_series_covid19_deaths_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the JHU COVID Case Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
=======
   "execution_count": 7,
>>>>>>> Stashed changes
=======
   "execution_count": 7,
>>>>>>> Stashed changes
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_pre_us = pd.read_csv(f'{data_path}/time_series_covid19_confirmed_US.csv')\n",
    "# death_pre_us = pd.read_csv(f'{data_path}/time_series_covid19_deaths_US.csv')\n",
    "\n",
    "\n",
    "file_list = os.listdir(out_data_path)\n",
    "for files in file_list:\n",
    "    if files.find('.xlsx') >= 0:\n",
    "        shutil.move(f'{out_data_path}/{files}',f'{out_data_path}/ARCHIVE/{files}')\n",
    "def DF_Transform(df, outcome):\n",
    "    global data_path\n",
    "    global out_data_path\n",
    "    \n",
    "    # Data Cleaning\n",
    "    df.drop(labels={'Lat','Long'},axis=1, inplace = True)\n",
    "#     df.loc[df['Country/Region'].str.contains('Congo'), 'Country/Region'] ='Congo'\n",
    "    df.loc[df['Country/Region'].str.contains('Korea, South',flags= re.IGNORECASE), 'Country/Region']= 'South Korea'\n",
    "    df.loc[df['Country/Region'] == ('US'), 'Country/Region']= 'United States'\n",
    "    df.loc[df['Country/Region'].str.contains('taiwan',flags= re.IGNORECASE), 'Country/Region']= 'taiwan'\n",
    "    # Data Manipulation\n",
    "    df = df.groupby(by='Country/Region').sum().T.apply(lambda x: x-x.shift(1),axis=0)\n",
    "    df.rename(columns={'Country/Region':'Date'},inplace=True)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df = df.reset_index()\n",
    "    df.rename(columns={'index':'date'},inplace= True)\n",
    "    df.date = pd.to_datetime(df.date).dt.date\n",
    "#     df['var'] = outcome[16:]\n",
    "    \n",
    "    filename = f'{out_data_path}/{outcome} (Through {df.date.max()}).xlsx'\n",
    "    df.to_excel(filename, index=False)\n",
    "    return df\n",
    "case = DF_Transform(case_pre, 'Global COVID-19 Case Count')\n",
    "death = DF_Transform(death_pre, 'Global COVID-19 Death Count')\n",
    "# case = DF_Transform(case_pre_us, 'US COVID-19 Case Count')\n",
    "# death = DF_Transform(death_pre_us, 'US COVID-19 Death Count')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_maker(dset, var):\n",
    "    name_list = dset.columns.to_list()[1:]\n",
    "    long_df = pd.DataFrame(columns = {'date', var, 'country'})\n",
    "    for name in name_list:\n",
    "        df = dset.filter(items={name, 'date'})\n",
    "        df['country'] = name\n",
    "        df.rename(columns={name:var},inplace = True)\n",
    "        long_df = pd.concat([long_df, df],axis=0)\n",
    "    long_df.date = pd.to_datetime(long_df.date)\n",
    "    return long_df.reset_index(drop=True)"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_lag_d = case.set_index('date')\n",
    "case_lag1 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-1),axis=0).reset_index()\n",
    "case_lag2 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-2),axis=0).reset_index()\n",
    "case_lag3 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-3),axis=0).reset_index()\n",
    "case_lag4 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-4),axis=0).reset_index()\n",
    "case_lag5 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-5),axis=0).reset_index()\n",
    "case_lag6 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-6),axis=0).reset_index()\n",
    "case_lag7 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-7),axis=0).reset_index()\n",
    "case_lag8 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-8),axis=0).reset_index()\n",
    "case_lag9 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-9),axis=0).reset_index()\n",
    "case_lag10 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-10),axis=0).reset_index()\n",
    "case_lag11 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-11),axis=0).reset_index()\n",
    "case_lag12 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-12),axis=0).reset_index()\n",
    "case_lag13 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-13),axis=0).reset_index()\n",
    "case_lag14 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-14),axis=0).reset_index()\n",
    "case_lag15 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-15),axis=0).reset_index()\n",
    "case_lag16 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-16),axis=0).reset_index()\n",
    "case_lag17 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-17),axis=0).reset_index()\n",
    "case_lag18 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-18),axis=0).reset_index()\n",
    "case_lag19 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-19),axis=0).reset_index()\n",
    "case_lag20 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-20),axis=0).reset_index()\n",
    "case_lag21 = case_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-21),axis=0).reset_index()"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 5,
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_lag_d = death.set_index('date')\n",
    "death_lag1 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-1),axis=0).reset_index()\n",
    "death_lag2 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-2),axis=0).reset_index()\n",
    "death_lag3 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-3),axis=0).reset_index()\n",
    "death_lag4 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-4),axis=0).reset_index()\n",
    "death_lag5 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-5),axis=0).reset_index()\n",
    "death_lag6 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-6),axis=0).reset_index()\n",
    "death_lag7 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-7),axis=0).reset_index()\n",
    "death_lag8 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-8),axis=0).reset_index()\n",
    "death_lag9 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-9),axis=0).reset_index()\n",
    "death_lag10 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-10),axis=0).reset_index()\n",
    "death_lag11 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-11),axis=0).reset_index()\n",
    "death_lag12 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-12),axis=0).reset_index()\n",
    "death_lag13 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-13),axis=0).reset_index()\n",
    "death_lag14 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-14),axis=0).reset_index()\n",
    "death_lag15 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-15),axis=0).reset_index()\n",
    "death_lag16 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-16),axis=0).reset_index()\n",
    "death_lag17 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-17),axis=0).reset_index()\n",
    "death_lag18 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-18),axis=0).reset_index()\n",
    "death_lag19 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-19),axis=0).reset_index()\n",
    "death_lag20 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-20),axis=0).reset_index()\n",
    "death_lag21 = death_lag_d.iloc[0:,1:].apply(lambda x: x.shift(-21),axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_case = long_maker(case, 'case_count')\n",
    "long_death = long_maker(death, 'death_count')"
   ]
  },
  {
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the Oxford Data Set"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_1 = long_maker(case_lag1,'case1')\n",
    "long_2 = long_maker(case_lag2,'case2')\n",
    "long_3 = long_maker(case_lag3,'case3')\n",
    "long_4 = long_maker(case_lag4,'case4')\n",
    "long_5 = long_maker(case_lag5,'case5')\n",
    "long_6 = long_maker(case_lag6,'case6')\n",
    "long_7 = long_maker(case_lag7,'case7')\n",
    "long_8 = long_maker(case_lag8,'case8')\n",
    "long_9 = long_maker(case_lag9,'case9')\n",
    "long_10 = long_maker(case_lag10,'case10')\n",
    "long_11 = long_maker(case_lag11,'case11')\n",
    "long_12 = long_maker(case_lag12,'case12')\n",
    "long_13 = long_maker(case_lag13,'case13')\n",
    "long_14 = long_maker(case_lag14,'case14')\n",
    "long_15 = long_maker(case_lag15,'case15')\n",
    "long_16 = long_maker(case_lag16,'case16')\n",
    "long_17 = long_maker(case_lag17,'case17')\n",
    "long_18 = long_maker(case_lag18,'case18')\n",
    "long_19 = long_maker(case_lag19,'case19')\n",
    "long_20 = long_maker(case_lag20,'case20')\n",
    "long_21 = long_maker(case_lag21,'case21')"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correcting_Col_Names(infile_path, dset):\n",
    "    infile = pd.read_csv(infile_path)\n",
    "    # og_col_list = infile.columns.to_list()\n",
    "    df = pd.DataFrame(infile.columns.to_list())\n",
    "    df['col'] = df[2:].apply(lambda x: pd.to_datetime(x).dt.strftime('X%m/X%d/%Y').str.replace('X0','').str.replace('X',''))\n",
    "    df['col'][0] = 'country'\n",
    "    df['col'][1] = 'country_code'\n",
    "    cols = df.col.to_list()\n",
    "    control = pd.read_csv(infile_path, names = cols, skiprows={0:1})\n",
    "    indexNames = control[ control['country_code'].isna()].index\n",
    "    control.drop(indexNames, inplace=True)\n",
    "    df = controls_transpose(control, dset)\n",
    "    return df"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_long_1 = long_maker(death_lag1,'death1')\n",
    "death_long_2 = long_maker(death_lag2,'death2')\n",
    "death_long_3 = long_maker(death_lag3,'death3')\n",
    "death_long_4 = long_maker(death_lag4,'death4')\n",
    "death_long_5 = long_maker(death_lag5,'death5')\n",
    "death_long_6 = long_maker(death_lag6,'death6')\n",
    "death_long_7 = long_maker(death_lag7,'death7')\n",
    "death_long_8 = long_maker(death_lag8,'death8')\n",
    "death_long_9 = long_maker(death_lag9,'death9')\n",
    "death_long_10 = long_maker(death_lag10,'death10')\n",
    "death_long_11 = long_maker(death_lag11,'death11')\n",
    "death_long_12 = long_maker(death_lag12,'death12')\n",
    "death_long_13 = long_maker(death_lag13,'death13')\n",
    "death_long_14 = long_maker(death_lag14,'death14')\n",
    "death_long_15 = long_maker(death_lag15,'death15')\n",
    "death_long_16 = long_maker(death_lag16,'death16')\n",
    "death_long_17 = long_maker(death_lag17,'death17')\n",
    "death_long_18 = long_maker(death_lag18,'death18')\n",
    "death_long_19 = long_maker(death_lag19,'death19')\n",
    "death_long_20 = long_maker(death_lag20,'death20')\n",
    "death_long_21 = long_maker(death_lag21,'death21')"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controls_transpose(dset, var):\n",
    "    import re\n",
    "    df = dset.T\n",
    "    df.columns = df.iloc[0]\n",
    "    df.columns =df.columns.str.lower()\n",
    "    df.drop(axis=0, index = {'country', 'country_code'},inplace = True)\n",
    "    df = df.reset_index().rename(columns={'index':'date'})\n",
    "    df = long_maker(df, var)\n",
    "    df.loc[df['country'].str.contains('cape verde',flags= re.IGNORECASE), 'country']= 'cabo verde'\n",
    "    df.loc[df['country'].str.contains('taiwan',flags= re.IGNORECASE), 'country']= 'taiwan'\n",
    "    return df"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_long_list = [case_lag1,case_lag2,case_lag3,case_lag4,case_lag5,case_lag6,\n",
    "#            case_lag7,case_lag8,case_lag9,case_lag10,case_lag11,case_lag12,\n",
    "#            case_lag13,case_lag14,case_lag15,case_lag16,case_lag17,case_lag18,\n",
    "#            case_lag19,case_lag20,case_lag21,death_lag1,death_lag2,death_lag3,\n",
    "#            death_lag4,death_lag5,death_lag6,death_lag7,death_lag8,death_lag9,\n",
    "#            death_lag10,death_lag11,death_lag12,death_lag13,death_lag14,\n",
    "#            death_lag15,death_lag16,death_lag17,death_lag18,death_lag19,death_lag20,\n",
    "#            death_lag21,death_long_1,death_long_2,death_long_3,death_long_4,death_long_5,\n",
    "#            death_long_6,death_long_7,death_long_8,death_long_9,death_long_10,death_long_11,\n",
    "#            death_long_12,death_long_13,death_long_14,death_long_15,death_long_16,death_long_17,\n",
    "#            death_long_18,death_long_19,death_long_20,death_long_21,long_1,long_2,long_3,long_4,\n",
    "#            long_5,long_6,long_7,long_8,long_9,long_10,long_11,long_12,long_13,long_14,\n",
    "#            long_15,long_16,long_17,long_18,long_19,long_20,long_21]"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 8,
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = partial(pd.merge,on=['date', 'country'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "c_measures_path = r'../../../covid-policy-tracker/data'\n",
    "school = Correcting_Col_Names(f'{c_measures_path}/timeseries/c1_schoolclosing.csv', 'school_close')\n",
    "work = Correcting_Col_Names(f'{c_measures_path}/timeseries/c2_workplaceclosing.csv', 'work_close')\n",
    "pub_events = Correcting_Col_Names(f'{c_measures_path}/timeseries/c3_cancelpublicevents.csv', 'public_events')\n",
    "gatherings = Correcting_Col_Names(f'{c_measures_path}/timeseries/c4_restrictionsongatherings.csv', 'large_gather')\n",
    "pub_transpo = Correcting_Col_Names(f'{c_measures_path}/timeseries/c5_closepublictransport.csv', 'public_transpo')\n",
    "stay_home = Correcting_Col_Names(f'{c_measures_path}/timeseries/c6_stayathomerequirements.csv' ,'stay_home')\n",
    "domestic_travel = Correcting_Col_Names(f'{c_measures_path}/timeseries/c7_domestictravel.csv' ,'domestic_travel')\n",
    "int_travel = Correcting_Col_Names(f'{c_measures_path}/timeseries/c8_internationaltravel.csv' ,'internat_travel')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 9,
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_case.merge(long_death, on=['date', 'country'])\n",
    "df_list = [long_df,school, work,pub_events,gatherings,pub_transpo,\n",
    "           stay_home,domestic_travel,int_travel,case_lag1,case_lag2,case_lag3,case_lag4,case_lag5,case_lag6,\n",
    "           case_lag7,case_lag8,case_lag9,case_lag10,case_lag11,case_lag12,\n",
    "           case_lag13,case_lag14,case_lag15,case_lag16,case_lag17,case_lag18,\n",
    "           case_lag19,case_lag20,case_lag21,death_lag1,death_lag2,death_lag3,\n",
    "           death_lag4,death_lag5,death_lag6,death_lag7,death_lag8,death_lag9,\n",
    "           death_lag10,death_lag11,death_lag12,death_lag13,death_lag14,\n",
    "           death_lag15,death_lag16,death_lag17,death_lag18,death_lag19,death_lag20,\n",
    "           death_lag21,death_long_1,death_long_2,death_long_3,death_long_4,death_long_5,\n",
    "           death_long_6,death_long_7,death_long_8,death_long_9,death_long_10,death_long_11,\n",
    "           death_long_12,death_long_13,death_long_14,death_long_15,death_long_16,death_long_17,\n",
    "           death_long_18,death_long_19,death_long_20,death_long_21,long_1,long_2,long_3,long_4,\n",
    "           long_5,long_6,long_7,long_8,long_9,long_10,long_11,long_12,long_13,long_14,\n",
    "           long_15,long_16,long_17,long_18,long_19,long_20,long_21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
      "text/plain": [
       "Timestamp('2020-10-21 00:00:00')"
      ]
     },
     "execution_count": 9,
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>case_count</th>\n",
       "      <th>country</th>\n",
       "      <th>death_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52348</th>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>3.0</td>\n",
       "      <td>taiwan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52349</th>\n",
       "      <td>2020-10-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>taiwan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52350</th>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>taiwan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52351</th>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>taiwan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52352</th>\n",
       "      <td>2020-10-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>taiwan</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52353 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  case_count      country  death_count\n",
       "0     2020-01-22         NaN  afghanistan          NaN\n",
       "1     2020-01-23         0.0  afghanistan          0.0\n",
       "2     2020-01-24         0.0  afghanistan          0.0\n",
       "3     2020-01-25         0.0  afghanistan          0.0\n",
       "4     2020-01-26         0.0  afghanistan          0.0\n",
       "...          ...         ...          ...          ...\n",
       "52348 2020-10-20         3.0       taiwan          0.0\n",
       "52349 2020-10-21         1.0       taiwan          0.0\n",
       "52350 2020-10-22         4.0       taiwan          0.0\n",
       "52351 2020-10-23         0.0       taiwan          0.0\n",
       "52352 2020-10-24         2.0       taiwan          0.0\n",
       "\n",
       "[52353 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "school.date.max()"
=======
    "long_df"
>>>>>>> Stashed changes
=======
    "long_df"
>>>>>>> Stashed changes
=======
    "long_df"
>>>>>>> Stashed changes
=======
    "long_df"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = long_case.merge(long_death, on=['date', 'country'])\n",
    "long_df = long_df.merge(school, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(work, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(pub_events, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(gatherings, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(pub_transpo, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(stay_home, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(domestic_travel, on=['date', 'country'],how='outer')\n",
    "long_df = long_df.merge(int_travel, on=['date', 'country'],how='outer')\n",
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'country'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b5aae0ef3052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlong_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1690\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'country'"
     ]
    }
   ],
   "source": [
    "long_df = reduce(merge,df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_long_1.groupby('date').sum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfs in df_list:\n",
    "    del dfs\n",
    "for dfs in df_list:\n",
    "    del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_long_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old merge statment\n",
    "# long_df = long_case.merge(long_death, on=['date', 'country'])\n",
    "# long_df = df_merger(school)\n",
    "# long_df = df_merger(work)\n",
    "# long_df = df_merger(pub_events)\n",
    "# long_df = df_merger(gatherings)\n",
    "# long_df = df_merger(pub_transpo)\n",
    "# long_df = df_merger(stay_home)\n",
    "# long_df = df_merger(domestic_travel)\n",
    "# long_df = df_merger(int_travel)\n",
    "# #Merging Long Case DFs\n",
    "# long_df = df_merger(long_1)\n",
    "# long_df = df_merger(long_2)\n",
    "# long_df = df_merger(long_3)\n",
    "# long_df = df_merger(long_4)\n",
    "# long_df = df_merger(long_5)\n",
    "# long_df = df_merger(long_6)\n",
    "# long_df = df_merger(long_7)\n",
    "# long_df = df_merger(long_8)\n",
    "# long_df = df_merger(long_9)\n",
    "# long_df = df_merger(long_10)\n",
    "# long_df = df_merger(long_11)\n",
    "# long_df = df_merger(long_12)\n",
    "# long_df = df_merger(long_13)\n",
    "# long_df = df_merger(long_14)\n",
    "# long_df = df_merger(long_15)\n",
    "# long_df = df_merger(long_16)\n",
    "# long_df = df_merger(long_17)\n",
    "# long_df = df_merger(long_18)\n",
    "# long_df = df_merger(long_19)\n",
    "# long_df = df_merger(long_20)\n",
    "# long_df = df_merger(long_21)\n",
    "# #Merging the long death vars\n",
    "# long_df = df_merger(death_long_1)\n",
    "# long_df = df_merger(death_long_2)\n",
    "# long_df = df_merger(death_long_3)\n",
    "# long_df = df_merger(death_long_4)\n",
    "# long_df = df_merger(death_long_5)\n",
    "# long_df = df_merger(death_long_6)\n",
    "# long_df = df_merger(death_long_7)\n",
    "# long_df = df_merger(death_long_8)\n",
    "# long_df = df_merger(death_long_9)\n",
    "# long_df = df_merger(death_long_10)\n",
    "# long_df = df_merger(death_long_11)\n",
    "# long_df = df_merger(death_long_12)\n",
    "# long_df = df_merger(death_long_13)\n",
    "# long_df = df_merger(death_long_14)\n",
    "# long_df = df_merger(death_long_15)\n",
    "# long_df = df_merger(death_long_16)\n",
    "# long_df = df_merger(death_long_17)\n",
    "# long_df = df_merger(death_long_18)\n",
    "# long_df = df_merger(death_long_19)\n",
    "# long_df = df_merger(death_long_20)\n",
    "# long_df = df_merger(death_long_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "long_df[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']] = long_df[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']].apply(lambda x: x.str.replace('.','999'))\n",
    "long_df[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']] = long_df[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']].fillna(999)\n",
    "long_df[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']] = long_df[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "       'stay_home', 'domestic_travel', 'internat_travel']].astype(int)"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "       'stay_home', 'domestic_travel', 'internat_travel']].astype(int)\n",
    "long_df.loc[long_df.school_close == 999, 'school_close'] = np.nan\n",
    "long_df.loc[long_df.work_close == 999, 'work_close'] = np.nan\n",
    "long_df.loc[long_df.public_events == 999, 'public_events'] = np.nan\n",
    "long_df.loc[long_df.large_gather == 999, 'large_gather'] = np.nan\n",
    "long_df.loc[long_df.public_transpo == 999, 'public_transpo'] = np.nan\n",
    "long_df.loc[long_df.stay_home == 999, 'stay_home'] = np.nan\n",
    "long_df.loc[long_df.domestic_travel == 999, 'domestic_travel'] = np.nan\n",
    "long_df.loc[long_df.internat_travel == 999, 'internat_travel'] = np.nan\n",
    "long_df[['school_close','work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']] = long_df.groupby('country')[['school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel']].apply(lambda x: x.ffill())"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 11,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summary = long_df.groupby('country').case_count.agg(['mean','median','std', 'max'])\n",
    "death_summary = long_df.groupby('country').case_count.agg(['mean','median','std', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Control Variables"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 12,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "control_var_path = '../Control Data'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 13,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_name_clean(df):\n",
    "    import re\n",
    "#     df.loc[(df['country'].str.contains('Congo',re.IGNORECASE)) & (~df['country'].str.contains('dem',re.IGNORECASE)) , 'country'] ='congo'\n",
    "    df.loc[(df['country'].str.contains('korea, s',flags= re.IGNORECASE)) & (df['country'].str.contains('south',flags= re.IGNORECASE)), 'country']= 'south korea'\n",
    "    df.loc[df['country'].str.contains('rep')& (df['country'].str.contains('congo',flags= re.IGNORECASE)) ,'country']= 'democratic republic of congo'\n",
    "    df.loc[df['country'] == ('US'), 'country']= 'united states'\n",
    "    df.loc[df['country'].str.contains('ivoire|ivory coast',flags= re.IGNORECASE), 'country']= 'cote d\\'ivoire'\n",
    "    df.loc[df['country'].str.contains('venezuela',flags= re.IGNORECASE), 'country']= 'venezuela'\n",
    "    df.loc[df['country'].str.contains('and principe',flags= re.IGNORECASE), 'country']= 'sao tome and principe'\n",
    "    df.loc[df['country'].str.contains('and the grenadines',flags= re.IGNORECASE), 'country']= 'saint vincent and the grenadines'\n",
    "    df.loc[df['country'].str.contains('kitts and nevis',flags= re.IGNORECASE), 'country']= 'saint kitts and nevis'\n",
    "    df.loc[df['country'].str.contains('bahamas',flags= re.IGNORECASE), 'country']= 'bahamas'\n",
    "    df.loc[df['country'].str.contains('yemen',flags= re.IGNORECASE), 'country']= 'yemen'\n",
    "    df.loc[df['country'].str.contains('gambia',flags= re.IGNORECASE), 'country']= 'gambia'\n",
    "    df.loc[df['country'].str.contains('hong kong',flags= re.IGNORECASE), 'country']= 'hong kong'\n",
    "    df.loc[df['country'].str.contains('macao',flags= re.IGNORECASE), 'country']= 'macao'\n",
    "    df.loc[df['country'].str.contains('iran',flags= re.IGNORECASE), 'country']= 'iran'\n",
    "    df.loc[df['country'].str.contains('lucia',flags= re.IGNORECASE), 'country']= 'saint lucia'\n",
    "    df.loc[df['country'].str.contains('lao pdr',flags= re.IGNORECASE), 'country']= 'laos'\n",
    "    df.loc[df['country'].str.contains('egypt',flags= re.IGNORECASE), 'country']= 'egypt'\n",
    "    df.loc[df['country'].str.contains('korea, rep.',flags= re.IGNORECASE), 'country']= 'south korea'\n",
    "    df.loc[df['country'].str.contains('states of america',flags= re.IGNORECASE), 'country']= 'united states'\n",
    "    df.loc[df['country'].str.contains('east timor',flags= re.IGNORECASE), 'country']= 'timor-leste'\n",
    "    df.loc[df['country'].str.contains('russia',flags= re.IGNORECASE), 'country']= 'russia'\n",
    "    df.loc[df['country'].str.contains('brunei',flags= re.IGNORECASE), 'country']= 'brunei'\n",
    "    df.loc[df['country'].str.contains('korea, dem. people\\'s rep',flags= re.IGNORECASE), 'country']= 'north korea'\n",
    "    dset = df.copy()\n",
    "    return dset\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "#COVID Regional Safety Assessment Data\n",
    "safety = pd.read_excel(f'{control_var_path}/COVID-19_Regional_Safety_Assessment.xlsx')\n",
    "safety.columns = safety.columns.str.lower()\n",
    "safety.rename(columns={'country/ region':'country'}, inplace=True)\n",
    "safety.country = safety.country.str.lower()\n",
    "safety = country_name_clean(safety)\n",
    "\n",
    "#World Bank GDP Data\n",
    "gdp = pd.read_excel(f'{control_var_path}/Global GDP.xlsx')\n",
    "gdp.drop(columns='Unnamed: 4', inplace=True)\n",
    "gdp.columns = gdp.columns.str.lower()\n",
    "gdp.rename(columns={'economy':'country'}, inplace=True)\n",
    "gdp.country = gdp.country.str.lower()\n",
    "\n",
    "og_data = pd.read_excel('..\\..\\Country Response Paper\\Original Documents\\Country Responses-selected\\Country Responses Dataset 7.28.20.xlsx',sheet_name = 'Country Responses')\n",
    "og_data.index = og_data['Country/Region']\n",
    "og_data = og_data.iloc[0:,19:]\n",
    "og_data = og_data.reset_index(drop=False).rename(columns={'Country/Region':'country'})\n",
    "og_data.columns = og_data.columns.str.lower()\n",
    "og_data = stats_col_renamer(og_data)\n",
    "og_data.dropna(how='all',inplace=True)\n",
    "og_data.country = og_data.country.str.lower()\n",
    "\n",
    "safety = country_name_clean(safety)\n",
    "gdp = country_name_clean(gdp)\n",
    "og_data = country_name_clean(og_data)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'population', 'ages_65_and_above__of_total_population',\n",
       "       'female__of_total', 'diabetes_prevalence__of_population_ages_20_to_79',\n",
       "       'obese__of_adult_population', 'htn_prevalence',\n",
       "       'smoking_prevalence_ages_15', 'cancer_prevalence_',\n",
       "       'hiv_prevalence__of_population_ages_15_49', 'copd_dalys_per_100000',\n",
       "       'sars_experience_0_no_1_yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "source": [
    "og_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and evaluating the smoking dataset from Our World Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 16,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_world_importer(filename,sheet, year=2017):\n",
    "    df = pd.read_excel(f'{control_data_path}\\\\{filename}.xlsx', sheet_name = sheet)\n",
    "    df = stats_col_renamer(df)\n",
    "    df = df.loc[df.year == year]\n",
    "    df.drop(columns=['entity','year'],inplace=True)\n",
    "    df.dropna(how='any',inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 17,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smoking Data\n",
    "smk_die_rate = our_world_importer('share-deaths-smoking', 'death-rate-smoking')\n",
    "smk_die_prop = our_world_importer('share-deaths-smoking', 'share-deaths-smoking')\n",
    "#HIV Data\n",
    "hiv_prev = our_world_importer('hiv-data','share-of-population-infected-wi')\n",
    "hiv_death = our_world_importer('hiv-data','hiv-death-rates')\n",
    "#Obesity Data\n",
    "obese = our_world_importer('share-of-deaths-obesity','share-of-deaths-obesity')\n",
    "#Population Data\n",
    "pop = our_world_importer('projected-population-by-country', 'projected-population-by-country',year=2020)\n",
    "#Age Data\n",
    "age = our_world_importer('median-age','median-age',year=2020)\n",
    "#Diabetes Data\n",
    "diabet = our_world_importer('diabetes-prevalence', 'diabetes-prevalence')\n",
    "mers = pd.read_excel(f'{control_data_path}\\MERS-SARS.xlsx',sheet_name='MERS')\n",
    "sars = pd.read_excel(f'{control_data_path}\\MERS-SARS.xlsx',sheet_name='SARS')\n",
    "mers = stats_col_renamer(mers)\n",
    "sars = stats_col_renamer(sars)\n",
    "sars.drop(columns=['number_of_hcw_affected_', 'date_onset_first_probable_case',\n",
    "       'date_onset_last_probable_case','case_fatality_ratio_','female', 'male','areas','median_age_range'],inplace=True)\n",
    "mers.drop(columns='country',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Merge a lot of DataFrames\n",
    "> Method 1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [smk_die_rate, smk_die_prop,hiv_prev, hiv_death, obese, pop, age, diabet, mers, sars]\n",
    "merge = partial(pd.merge,on='code', how='outer')\n",
    "control = reduce(merge,dfs)\n",
    "control[['mers_case_count',\n",
    "       'sars_case_count', 'number_of_deathsa', 'number_of_imported_cases',\n",
    "       'percent_of_imported_cases']] = control[['mers_case_count',\n",
    "       'sars_case_count', 'number_of_deathsa', 'number_of_imported_cases',\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "       'percent_of_imported_cases']].fillna(0)\n",
    "control['mers_sars_max'] = control[['mers_case_count', 'sars_case_count']].max(axis=1)\n",
    "control['mers_sars_sum'] = control[['mers_case_count', 'sars_case_count']].sum(axis=1)\n",
    "mers_sars_sum =  control.loc[control.mers_sars_sum > 0 ]['mers_sars_sum']\n",
    "des_mers_sum = mers_sars_sum.describe().to_frame()\n",
    "q1 = int(des_mers_sum.iloc[4].values)\n",
    "q2 = int(des_mers_sum.iloc[5].values)\n",
    "q3 = int(des_mers_sum.iloc[6].values)\n",
    "#creating the quartile variable\n",
    "control['mers_sars_quart'] = 0\n",
    "control.loc[(control.mers_sars_sum > 0) & (control.mers_sars_sum <= q1), 'mers_sars_quart']=1\n",
    "control.loc[(control.mers_sars_sum > q1) & (control.mers_sars_sum <= q2), 'mers_sars_quart']=2\n",
    "control.loc[(control.mers_sars_sum > q2) & (control.mers_sars_sum <= q3), 'mers_sars_quart']=3\n",
    "control.loc[control.mers_sars_sum > q3, 'mers_sars_quart']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x192cbbd4f88>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "control.mers_sars_quart.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10545.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control.mers_sars_sum.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2         2.0\n",
       "9         6.0\n",
       "10        2.0\n",
       "13        1.0\n",
       "32      251.0\n",
       "37     5328.0\n",
       "53        1.0\n",
       "61        9.0\n",
       "65       12.0\n",
       "67        1.0\n",
       "79        3.0\n",
       "80        2.0\n",
       "81        6.0\n",
       "83        1.0\n",
       "85        5.0\n",
       "88       28.0\n",
       "92        5.0\n",
       "96        2.0\n",
       "105       7.0\n",
       "115       9.0\n",
       "122       2.0\n",
       "123       1.0\n",
       "130      24.0\n",
       "137      16.0\n",
       "141      19.0\n",
       "142       1.0\n",
       "143       1.0\n",
       "149    2058.0\n",
       "154     238.0\n",
       "159       1.0\n",
       "160     189.0\n",
       "162       1.0\n",
       "167       5.0\n",
       "168       1.0\n",
       "170     346.0\n",
       "173      12.0\n",
       "178       3.0\n",
       "179       1.0\n",
       "183      87.0\n",
       "184       9.0\n",
       "185      29.0\n",
       "191      63.0\n",
       "193       1.0\n",
       "210    1755.0\n",
       "213       1.0\n",
       "Name: mers_sars_sum, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mers_sars_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     237.000000\n",
       "mean       44.493671\n",
       "std       387.976078\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max      5328.000000\n",
       "Name: mers_sars_sum, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control.mers_sars_sum.describe()"
=======
    "       'percent_of_imported_cases']].fillna(0)"
>>>>>>> Stashed changes
=======
    "       'percent_of_imported_cases']].fillna(0)"
>>>>>>> Stashed changes
=======
    "       'percent_of_imported_cases']].fillna(0)"
>>>>>>> Stashed changes
=======
    "       'percent_of_imported_cases']].fillna(0)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Method 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# control = smk_die_rate.copy()\n",
    "# control = control.merge(smk_die_prop, on='code', how='outer')\n",
    "# control = control.merge(hiv_prev, on='code', how='outer')\n",
    "# control = control.merge(hiv_death, on='code', how='outer')\n",
    "# control = control.merge(obese, on='code', how='outer')\n",
    "# control = control.merge(pop, on='code', how='outer')\n",
    "# control = control.merge(age, on='code', how='outer')\n",
    "# control = control.merge(diabet, on='code', how='outer')\n",
    "# control = control.merge(mers, on='code', how='outer')\n",
    "# control = control.merge(sars, on='code', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Merges"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 25,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol = control.merge(gdp,left_on='code',right_on='countrycode',how='outer')\n",
    "long_df_newcontrol = long_df_newcontrol.merge(long_df, on=['country'],how='outer')\n",
    "long_df_newcontrol = long_df_newcontrol.merge(safety, on=['country'],how='outer')\n",
    "long_df_newcontrol = long_df_newcontrol.merge(og_data, on=['country'],how='outer')\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "long_df_newcontrol.rename(columns={'us_dollars_in_mil':'gdp_in_mil_us', 'ranking':'gdp_rank'},inplace=True)\n",
    "long_df_newcontrol = stats_col_renamer(long_df_newcontrol)\n",
=======
    "long_df_newcontrol = stats_col_renamer(long_df_newcontrol)\n",
    "long_df_newcontrol.rename(columns={'us_dollars_in_mil':'gdp_in_mil_us', 'ranking':'gdp_rank'},inplace=True)\n",
>>>>>>> Stashed changes
=======
    "long_df_newcontrol = stats_col_renamer(long_df_newcontrol)\n",
    "long_df_newcontrol.rename(columns={'us_dollars_in_mil':'gdp_in_mil_us', 'ranking':'gdp_rank'},inplace=True)\n",
>>>>>>> Stashed changes
=======
    "long_df_newcontrol = stats_col_renamer(long_df_newcontrol)\n",
    "long_df_newcontrol.rename(columns={'us_dollars_in_mil':'gdp_in_mil_us', 'ranking':'gdp_rank'},inplace=True)\n",
>>>>>>> Stashed changes
=======
    "long_df_newcontrol = stats_col_renamer(long_df_newcontrol)\n",
    "long_df_newcontrol.rename(columns={'us_dollars_in_mil':'gdp_in_mil_us', 'ranking':'gdp_rank'},inplace=True)\n",
>>>>>>> Stashed changes
    "long_df_newcontrol.drop(columns={'global_region','code'},inplace=True)\n",
    "long_df_newcontrol.columns = long_df_newcontrol.columns.str.replace('__','_')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 26,
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_order = ['date',  'country','case_count', 'death_count', 'school_close',\n",
    "       'work_close', 'public_events', 'large_gather', 'public_transpo',\n",
    "       'stay_home', 'domestic_travel', 'internat_travel', 'population',\n",
    "       'ages_65_and_above_of_total_population', 'female_of_total',\n",
    "       'diabetes_prevalence_of_population_ages_20_to_79',\n",
    "       'obese_of_adult_population', 'htn_prevalence',\n",
    "       'smoking_prevalence_ages_15', 'cancer_prevalence_',\n",
    "       'hiv_prevalence_of_population_ages_1549', 'copd_dalys_per_100000',\n",
    "       'sars_experience_0_no_1_yes', 'quarantine_efficiency', 'gov_efficiency',\n",
    "       'monitoring_and_detection', 'healthcare_readiness',\n",
    "       'country_vulnerability', 'emergency_preparedness', 'total_score',\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "       'countrycode', 'gdp_rank', 'gdp_in_mil_us']"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "       'countrycode', 'gdp_rank', 'gdp_in_mil_us', 'case1', 'case2',\n",
    "       'case3', 'case4', 'case5', 'case6', 'case7', 'case8', 'case9', 'case10',\n",
    "       'case11', 'case12', 'case13', 'case14', 'case15', 'case16', 'case17',\n",
    "       'case18', 'case19', 'case20', 'case21', 'death1', 'death2', 'death3',\n",
    "       'death4', 'death5', 'death6', 'death7', 'death8', 'death9', 'death10',\n",
    "       'death11', 'death12', 'death13', 'death14', 'death15', 'death16',\n",
    "       'death17', 'death18', 'death19', 'death20', 'death21'\n",
    "]"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['country','countrycode','date','case_count','death_count',\n",
    "'school_close', 'domestic_travel','internat_travel','large_gather', 'public_events',\n",
    "'stay_home', 'work_close' 'public_transpo',\n",
    "'gdp_rank', 'us dollars in mil','smoking_ihme_2019', 'population_by_country_and_region_historic_and_projections_gapminder_hyde__un',\n",
    "'prevalence_hivaids_sex_both_age_15_49_years_percent', 'deaths_hivaids_sex_both_age_age_standardized_rate', 'deaths_smoking_sex_both_age_age_standardized_rate',\n",
    "'ages_65_and_above_of_total_population','htn_prevalence', 'copd_dalys_per_100000', 'obesity_ihme_2019',\n",
    "'country_vulnerability','emergency_preparedness','gov_efficiency','healthcare readiness',\n",
    "'monitoring and detection', 'quarantine_efficiency', 'total_score',\n",
    "'mers_case_count', 'sars_case_count', 'number_of_deathsa', 'number_of_imported_cases', 'percent_of_imported_cases']"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = ['country','countrycode','date','case_count','death_count',\n",
    "#      'school_close', 'domestic_travel','internat_travel','large_gather', 'public_events',\n",
    "#      'stay_home', 'work_close' 'public_transpo','gdp_rank','gdp_in_mil_us','smoking_ihme_2019',\n",
    "#      'population_by_country_and_region_historic_and_projections_gapminder_hyde__un',\n",
    "#      'prevalence_hivaids_sex_both_age_15_49_years_percent', \n",
    "#      'deaths_hivaids_sex_both_age_age_standardized_rate', 'deaths_smoking_sex_both_age_age_standardized_rate',\n",
    "#      'ages_65_and_above_of_total_population','htn_prevalence', 'copd_dalys_per_100000', 'obesity_ihme_2019',\n",
    "#      'country_vulnerability','emergency_preparedness','gov_efficiency','healthcare readiness',\n",
    "#      'monitoring and detection', 'quarantine_efficiency', 'total_score',\n",
    "#      'mers_case_count', 'sars_case_count', 'number_of_deathsa', \n",
    "#      'number_of_imported_cases', 'percent_of_imported_cases']"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 28,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.countrycode = long_df_newcontrol.countrycode.str.upper()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63777, 49)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "long_df_newcontrol.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['deaths_smoking_sex_both_age_age_standardized_rate',\n",
       "       'smoking_ihme_2019',\n",
       "       'prevalence_hivaids_sex_both_age_15_49_years_percent',\n",
       "       'deaths_hivaids_sex_both_age_age_standardized_rate',\n",
       "       'obesity_ihme_2019',\n",
       "       'population_by_country_and_region_historic_and_projections_gapminder_hyde_un',\n",
       "       'un_population_division_median_age_2017',\n",
       "       'diabetes_prevalence_of_population_ages_20_to_79_x', 'mers_case_count',\n",
       "       'sars_case_count', 'number_of_deathsa', 'number_of_imported_cases',\n",
       "       'percent_of_imported_cases', 'mers_sars_max', 'mers_sars_sum',\n",
       "       'mers_sars_quart', 'countrycode', 'gdp_rank', 'country',\n",
       "       'us_dollars_in_mil', 'case_count', 'date', 'death_count',\n",
       "       'school_close', 'work_close', 'public_events', 'large_gather',\n",
       "       'public_transpo', 'stay_home', 'domestic_travel', 'internat_travel',\n",
       "       'quarantine_efficiency', 'gov_efficiency', 'monitoring_and_detection',\n",
       "       'healthcare_readiness', 'country_vulnerability',\n",
       "       'emergency_preparedness', 'total_score', 'population',\n",
       "       'ages_65_and_above_of_total_population', 'female_of_total',\n",
       "       'diabetes_prevalence_of_population_ages_20_to_79_y',\n",
       "       'obese_of_adult_population', 'htn_prevalence',\n",
       "       'smoking_prevalence_ages_15', 'cancer_prevalence_',\n",
       "       'hiv_prevalence_of_population_ages_15_49', 'copd_dalys_per_100000',\n",
       "       'sars_experience_0_no_1_yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_df_newcontrol.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol = long_df_newcontrol.filter(items=['country','countrycode','date','case_count','death_count',\n",
    "'school_close', 'domestic_travel','internat_travel','large_gather', 'public_events',\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "'stay_home', 'work_close' 'public_transpo',\n",
    "'gdp_rank', 'us_dollars_in_mil','smoking_ihme_2019', 'population_by_country_and_region_historic_and_projections_gapminder_hyde_un',\n",
=======
    "'stay_home', 'work_close' ,'public_transpo',\n",
    "'gdp_rank', 'gdp_in_mil_us','smoking_ihme_2019', 'population_by_country_and_region_historic_and_projections_gapminder_hyde_un',\n",
>>>>>>> Stashed changes
=======
    "'stay_home', 'work_close' ,'public_transpo',\n",
    "'gdp_rank', 'gdp_in_mil_us','smoking_ihme_2019', 'population_by_country_and_region_historic_and_projections_gapminder_hyde_un',\n",
>>>>>>> Stashed changes
=======
    "'stay_home', 'work_close' ,'public_transpo',\n",
    "'gdp_rank', 'gdp_in_mil_us','smoking_ihme_2019', 'population_by_country_and_region_historic_and_projections_gapminder_hyde_un',\n",
>>>>>>> Stashed changes
=======
    "'stay_home', 'work_close' ,'public_transpo',\n",
    "'gdp_rank', 'gdp_in_mil_us','smoking_ihme_2019', 'population_by_country_and_region_historic_and_projections_gapminder_hyde_un',\n",
>>>>>>> Stashed changes
    "'un_population_division_median_age_2017','ages_65_and_above_of_total_population',\n",
    "'prevalence_hivaids_sex_both_age_15_49_years_percent',\n",
    "'deaths_hivaids_sex_both_age_age_standardized_rate', \n",
    "'diabetes_prevalence_of_population_ages_20_to_79_x',\n",
    "'deaths_smoking_sex_both_age_age_standardized_rate','cancer_prevalence_',\n",
    "'htn_prevalence', 'copd_dalys_per_100000', 'obesity_ihme_2019',\n",
    "'country_vulnerability','emergency_preparedness','gov_efficiency','healthcare readiness',\n",
    "'monitoring and detection', 'quarantine_efficiency', 'total_score',\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "'mers_case_count', 'sars_case_count', 'mers_sars_sum','mers_sars_quart','mers_sars_max' , 'number_of_deathsa', 'number_of_imported_cases', 'percent_of_imported_cases'])"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "'mers_case_count', 'sars_case_count', 'number_of_deathsa', 'number_of_imported_cases', 'percent_of_imported_cases',\n",
    "'case1', 'case2','case3', 'case4', 'case5', 'case6', 'case7', \n",
    "'case8', 'case9', 'case10','case11', 'case12', 'case13', 'case14', \n",
    "'case15', 'case16', 'case17','case18', 'case19', 'case20', \n",
    "'case21', 'death1', 'death2', 'death3','death4', 'death5', \n",
    "'death6', 'death7', 'death8', 'death9', 'death10','death11', \n",
    "'death12', 'death13', 'death14', 'death15', 'death16',\n",
    "       'death17', 'death18', 'death19', 'death20', 'death21'])"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.rename(columns={'us_dollars_in_mil':'gdp_in_mil_us', 'population_by_country_and_region_historic_and_projections_gapminder_hyde_un':'pop_2020',\n",
    "                                   'deaths_hivaids_sex_both_age_age_standardized_rate':'deaths_hivaids_sex_both_age',\n",
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.rename(columns={'population_by_country_and_region_historic_and_projections_gapminder_hyde_un':'pop_2020','deaths_hivaids_sex_both_age_age_standardized_rate':'deaths_hivaids_sex_both_age',\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "                                  'diabetes_prevalence_of_population_ages_20_to_79_x':'diabetes_prev_ages_20_to_79',\n",
    "                                  'cancer_prevalence_':'cancer_prevalence'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 33,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "date = long_df_newcontrol.date.max().date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating SARS/MERS Experience Variables"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 34,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.mers_case_count = long_df_newcontrol.mers_case_count.fillna(0)\n",
    "long_df_newcontrol.sars_case_count = long_df_newcontrol.sars_case_count.fillna(0)\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
    "long_df_newcontrol['mers_sars_max'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].max(axis=1)\n",
    "long_df_newcontrol['mers_sars_sum'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].sum(axis=1)\n",
>>>>>>> Stashed changes
=======
    "long_df_newcontrol['mers_sars_max'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].max(axis=1)\n",
    "long_df_newcontrol['mers_sars_sum'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].sum(axis=1)\n",
>>>>>>> Stashed changes
=======
    "long_df_newcontrol['mers_sars_max'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].max(axis=1)\n",
    "long_df_newcontrol['mers_sars_sum'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].sum(axis=1)\n",
>>>>>>> Stashed changes
=======
    "long_df_newcontrol['mers_sars_max'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].max(axis=1)\n",
    "long_df_newcontrol['mers_sars_sum'] = long_df_newcontrol[['mers_case_count', 'sars_case_count']].sum(axis=1)\n",
>>>>>>> Stashed changes
    "long_df_newcontrol['mers_sars_exp1'] = 0\n",
    "long_df_newcontrol['mers_sars_exp5'] = 0\n",
    "long_df_newcontrol['mers_sars_exp10'] = 0\n",
    "long_df_newcontrol['mers_sars_exp20'] = 0\n",
    "long_df_newcontrol['mers_sars_exp100'] = 0\n",
    "long_df_newcontrol['mers_sars_exp200'] = 0\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "long_df_newcontrol['mers_sars_exp300'] = 0\n",
    "long_df_newcontrol['mers_sars_exp400'] = 0\n",
    "long_df_newcontrol['mers_sars_exp500'] = 0\n",
    "long_df_newcontrol['mers_sars_exp1000'] = 0\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 1, 'mers_sars_exp1' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 5, 'mers_sars_exp5' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 10, 'mers_sars_exp10' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 20, 'mers_sars_exp20' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 100, 'mers_sars_exp100' ] = 1\n",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 200, 'mers_sars_exp200' ] = 1"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 200, 'mers_sars_exp200' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 300, 'mers_sars_exp300' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 400, 'mers_sars_exp400' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 500, 'mers_sars_exp500' ] = 1\n",
    "long_df_newcontrol.loc[long_df_newcontrol.mers_sars_max >= 1000, 'mers_sars_exp1000' ] = 1"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.to_excel(f'{out_data_path}\\\\Final COVID Data Set (Through {date}) (ver2).xlsx',index=False)"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol['dayoweek'] = long_df_newcontrol.date.dt.dayofweek\n",
    "long_df_newcontrol['week'] = long_df_newcontrol.date.dt.weekofyear\n",
    "long_df_newcontrol['month'] = long_df_newcontrol.date.dt.month"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x192cbbeca48>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_df_newcontrol.mers_sars_quart.hist()"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.columns"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "long_df_newcontrol.columns"
=======
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
    "long_df_week = pd.DataFrame()\n",
    "long_df_week = long_df_newcontrol[['country','case_count', 'death_count','week']].groupby(['country','week']).sum()\n",
    "con = long_df_newcontrol[['country','work_close','school_close', 'domestic_travel', \n",
    "                          'internat_travel', 'large_gather','public_events', \n",
    "                          'stay_home', 'gdp_rank','gdp_in_mil_us','week','smoking_ihme_2019',\n",
    "                          'pop_2020', 'un_population_division_median_age_2017',\n",
    "                          'ages_65_and_above_of_total_population',\n",
    "                          'prevalence_hivaids_sex_both_age_15_49_years_percent',\n",
    "                          'deaths_hivaids_sex_both_age', 'diabetes_prev_ages_20_to_79',\n",
    "                          'deaths_smoking_sex_both_age_age_standardized_rate',\n",
    "                          'cancer_prevalence', 'htn_prevalence', 'copd_dalys_per_100000',\n",
    "                          'obesity_ihme_2019', 'country_vulnerability', 'emergency_preparedness',\n",
    "                          'gov_efficiency', 'quarantine_efficiency', 'total_score',\n",
    "                          'mers_case_count', 'sars_case_count', 'number_of_deathsa',\n",
    "                          'number_of_imported_cases', 'percent_of_imported_cases',\n",
    "                          'mers_sars_max', 'mers_sars_sum', 'mers_sars_exp1', 'mers_sars_exp5',\n",
    "                          'mers_sars_exp10', 'mers_sars_exp20', 'mers_sars_exp100',\n",
    "                          'mers_sars_exp200','mers_sars_exp300','mers_sars_exp400','mers_sars_exp500',\n",
    "                          'mers_sars_exp1000']].groupby(['country','week']).apply(lambda x: x.mode())\n",
    "con.drop(columns=['week','country'], inplace=True)\n",
    "con.reset_index(inplace=True)\n",
    "long_df_week.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol.stay_home.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_week = long_df_week.merge(con, on=['country','week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df_newcontrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_week.to_excel(f'{out_data_path}\\\\Final Weekly COVID Data Set (Through {date}) (ver2).xlsx',index=False)\n",
    "long_df_newcontrol.to_excel(f'{out_data_path}\\\\Final COVID Data Set (Through {date}) (ver2).xlsx',index=False)"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "## trying to work out country linking stuff"
=======
    "# Trying to work out country linking stuff"
>>>>>>> Stashed changes
=======
    "# Trying to work out country linking stuff"
>>>>>>> Stashed changes
=======
    "# Trying to work out country linking stuff"
>>>>>>> Stashed changes
=======
    "# Trying to work out country linking stuff"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_check(q,l):\n",
    "    from fuzzywuzzy import fuzz,process\n",
    "    from Levenshtein import distance,ratio\n",
    "    import pandas as pd\n",
    "    jhu = []\n",
    "    similar = []\n",
    "    query = []\n",
    "    for countries in q:\n",
    "        results = process.extractOne(countries,list(jhu_country)) \n",
    "        if results[1] < 100:\n",
    "            jhu.append(results[0])\n",
    "            similar.append(results[1])\n",
    "            query.append(countries)\n",
    "    output = pd.DataFrame({'query_country':query, \n",
    "                          'similarity':similar,\n",
    "                           'jhu_country':jhu                       \n",
    "                          })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of the country names from each data set\n",
    "jhu_country = long_case.country.unique()\n",
    "ox_country = school.country.unique()\n",
    "gdp_country = gdp.country\n",
    "safe_country = safety.country\n",
    "og_country = og_data.country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Name Cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oxford Data Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox_check = similar_check(ox_country ,jhu_country)\n",
    "ox_check.sort_values(by='similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDP Data Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_check = similar_check(gdp_country ,jhu_country)\n",
    "gdp_check.sort_values(by='similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Data Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_check = similar_check(safe_country ,jhu_country)\n",
    "safe_check.sort_values(by='similarity', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OG Data Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_check = similar_check(og_country ,jhu_country)\n",
    "og_check.sort_values(by='similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Grave Yard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Oxford Control Measures Data Set (Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from fuzzywuzzy import fuzz,process\n",
    "# from Levenshtein import distance,ratio\n",
    "# import pandas as pd\n",
    "# jhu = []\n",
    "# similar = []\n",
    "# ox = []\n",
    "# for countries in ox_country:\n",
    "#     results = process.extractOne(countries,list(jhu_country)) \n",
    "#     if results[1] < 100:\n",
    "#         jhu.append(results[0])\n",
    "#         similar.append(results[1])\n",
    "#         ox.append(countries)\n",
    "# output = pd.DataFrame({'oxford_country':ox, \n",
    "#                       'similarity':similar,\n",
    "#                        'jhu_country':jhu                       \n",
    "#                       })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
